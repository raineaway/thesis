{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import pandas as pd\n",
    "import pprint\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of articles: 6299\n"
     ]
    }
   ],
   "source": [
    "corpus = pd.read_csv('gm_fake_or_real.csv');\n",
    "\n",
    "data = []\n",
    "labels = corpus['class'];\n",
    "for news in corpus['text']:\n",
    "    data.append(news)\n",
    "\n",
    "print \"Total number of articles: \" + str(len(data))\n",
    "\n",
    "div = len(corpus)*0.85\n",
    "div = int(floor(div))\n",
    "\n",
    "X_train = [data[i] for i in range(0,div)]\n",
    "X_test = [data[i] for i in range(div,len(corpus))]\n",
    "y_train, y_test = labels[:div], labels[div:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5354, 21517)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=5,\n",
    "                             max_df = 0.8,\n",
    "                             sublinear_tf=True,\n",
    "                             use_idf=True,\n",
    "                             stop_words='english',\n",
    "                             token_pattern=ur'(?u)\\b[^\\W\\d][^\\W\\d]+\\b')\n",
    "\n",
    "train_corpus_tf_idf = vectorizer.fit_transform(X_train) \n",
    "test_corpus_tf_idf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_tfidf = train_corpus_tf_idf.toarray()\n",
    "X_test_tfidf = test_corpus_tf_idf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer2 = CountVectorizer(min_df=5,\n",
    "                             max_df = 0.8,\n",
    "                             stop_words='english',\n",
    "                             token_pattern=ur'(?u)\\b[^\\W\\d][^\\W\\d]+\\b')\n",
    "\n",
    "train_corpus_count = vectorizer2.fit_transform(X_train) \n",
    "test_corpus_count = vectorizer2.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_count = train_corpus_count.toarray()\n",
    "X_test_count = test_corpus_count.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "\n",
    "\n",
    "class NeuralNetMLP(object):\n",
    "    \"\"\" Feedforward neural network / Multi-layer perceptron classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    n_output : int\n",
    "        Number of output units, should be equal to the\n",
    "        number of unique class labels.\n",
    "    n_features : int\n",
    "        Number of features (dimensions) in the target dataset.\n",
    "        Should be equal to the number of columns in the X array.\n",
    "    n_hidden : int (default: 30)\n",
    "        Number of hidden units.\n",
    "    l1 : float (default: 0.0)\n",
    "        Lambda value for L1-regularization.\n",
    "        No regularization if l1=0.0 (default)\n",
    "    l2 : float (default: 0.0)\n",
    "        Lambda value for L2-regularization.\n",
    "        No regularization if l2=0.0 (default)\n",
    "    epochs : int (default: 500)\n",
    "        Number of passes over the training set.\n",
    "    eta : float (default: 0.001)\n",
    "        Learning rate.\n",
    "    alpha : float (default: 0.0)\n",
    "        Momentum constant. Factor multiplied with the\n",
    "        gradient of the previous epoch t-1 to improve\n",
    "        learning speed\n",
    "        w(t) := w(t) - (grad(t) + alpha*grad(t-1))\n",
    "    decrease_const : float (default: 0.0)\n",
    "        Decrease constant. Shrinks the learning rate\n",
    "        after each epoch via eta / (1 + epoch*decrease_const)\n",
    "    shuffle : bool (default: True)\n",
    "        Shuffles training data every epoch if True to prevent circles.\n",
    "    minibatches : int (default: 1)\n",
    "        Divides training data into k minibatches for efficiency.\n",
    "        Normal gradient descent learning if k=1 (default).\n",
    "    random_state : int (default: None)\n",
    "        Set random state for shuffling and initializing the weights.\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    cost_ : list\n",
    "      Sum of squared errors after each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_output, n_features, n_hidden=30,\n",
    "                 l1=0.0, l2=0.0, epochs=500, eta=0.001,\n",
    "                 alpha=0.0, decrease_const=0.0, shuffle=True,\n",
    "                 minibatches=1, random_state=None):\n",
    "\n",
    "        np.random.seed(random_state)\n",
    "        self.n_output = n_output\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.w1, self.w2 = self._initialize_weights()\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "\n",
    "    def _encode_labels(self, y, k):\n",
    "        \"\"\"Encode labels into one-hot representation\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        y : array, shape = [n_samples]\n",
    "            Target values.\n",
    "\n",
    "        Returns\n",
    "        -----------\n",
    "        onehot : array, shape = (n_labels, n_samples)\n",
    "\n",
    "        \"\"\"\n",
    "        onehot = np.zeros((k, y.shape[0]))\n",
    "        for idx, val in enumerate(y):\n",
    "            onehot[val, idx] = 1.0\n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        w1 = np.random.uniform(-1.0, 1.0,\n",
    "                               size=self.n_hidden*(self.n_features + 1))\n",
    "        w1 = w1.reshape(self.n_hidden, self.n_features + 1)\n",
    "        w2 = np.random.uniform(-1.0, 1.0,\n",
    "                               size=self.n_output*(self.n_hidden + 1))\n",
    "        w2 = w2.reshape(self.n_output, self.n_hidden + 1)\n",
    "        return w1, w2\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Compute logistic function (sigmoid)\n",
    "\n",
    "        Uses scipy.special.expit to avoid overflow\n",
    "        error for very small input values z.\n",
    "\n",
    "        \"\"\"\n",
    "        # return 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "\n",
    "    def _sigmoid_gradient(self, z):\n",
    "        \"\"\"Compute gradient of the logistic function\"\"\"\n",
    "        sg = self._sigmoid(z)\n",
    "        return sg * (1.0 - sg)\n",
    "\n",
    "    def _add_bias_unit(self, X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            X_new = np.ones((X.shape[0], X.shape[1] + 1))\n",
    "            X_new[:, 1:] = X\n",
    "        elif how == 'row':\n",
    "            X_new = np.ones((X.shape[0] + 1, X.shape[1]))\n",
    "            X_new[1:, :] = X\n",
    "        else:\n",
    "            raise AttributeError('`how` must be `column` or `row`')\n",
    "        return X_new\n",
    "\n",
    "    def _feedforward(self, X, w1, w2):\n",
    "        \"\"\"Compute feedforward step\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        a1 : array, shape = [n_samples, n_features+1]\n",
    "            Input values with bias unit.\n",
    "        z2 : array, shape = [n_hidden, n_samples]\n",
    "            Net input of hidden layer.\n",
    "        a2 : array, shape = [n_hidden+1, n_samples]\n",
    "            Activation of hidden layer.\n",
    "        z3 : array, shape = [n_output_units, n_samples]\n",
    "            Net input of output layer.\n",
    "        a3 : array, shape = [n_output_units, n_samples]\n",
    "            Activation of output layer.\n",
    "\n",
    "        \"\"\"\n",
    "        a1 = self._add_bias_unit(X, how='column')\n",
    "        z2 = w1.dot(a1.T)\n",
    "        a2 = self._sigmoid(z2)\n",
    "        a2 = self._add_bias_unit(a2, how='row')\n",
    "        z3 = w2.dot(a2)\n",
    "        a3 = self._sigmoid(z3)\n",
    "        return a1, z2, a2, z3, a3\n",
    "\n",
    "    def _L2_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        return (lambda_/2.0) * (np.sum(w1[:, 1:] ** 2) +\n",
    "                                np.sum(w2[:, 1:] ** 2))\n",
    "\n",
    "    def _L1_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L1-regularization cost\"\"\"\n",
    "        return (lambda_/2.0) * (np.abs(w1[:, 1:]).sum() +\n",
    "                                np.abs(w2[:, 1:]).sum())\n",
    "\n",
    "    def _get_cost(self, y_enc, output, w1, w2):\n",
    "        \"\"\"Compute cost function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_enc : array, shape = (n_labels, n_samples)\n",
    "            one-hot encoded class labels.\n",
    "        output : array, shape = [n_output_units, n_samples]\n",
    "            Activation of the output layer (feedforward)\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        cost : float\n",
    "            Regularized cost.\n",
    "\n",
    "        \"\"\"\n",
    "        term1 = -y_enc * (np.log(output))\n",
    "        term2 = (1.0 - y_enc) * np.log(1.0 - output)\n",
    "        cost = np.sum(term1 - term2)\n",
    "        L1_term = self._L1_reg(self.l1, w1, w2)\n",
    "        L2_term = self._L2_reg(self.l2, w1, w2)\n",
    "        cost = cost + L1_term + L2_term\n",
    "        return cost\n",
    "\n",
    "    def _get_gradient(self, a1, a2, a3, z2, y_enc, w1, w2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        a1 : array, shape = [n_samples, n_features+1]\n",
    "            Input values with bias unit.\n",
    "        a2 : array, shape = [n_hidden+1, n_samples]\n",
    "            Activation of hidden layer.\n",
    "        a3 : array, shape = [n_output_units, n_samples]\n",
    "            Activation of output layer.\n",
    "        z2 : array, shape = [n_hidden, n_samples]\n",
    "            Net input of hidden layer.\n",
    "        y_enc : array, shape = (n_labels, n_samples)\n",
    "            one-hot encoded class labels.\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        grad1 : array, shape = [n_hidden_units, n_features]\n",
    "            Gradient of the weight matrix w1.\n",
    "        grad2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Gradient of the weight matrix w2.\n",
    "\n",
    "        \"\"\"\n",
    "        # backpropagation\n",
    "        sigma3 = a3 - y_enc\n",
    "        z2 = self._add_bias_unit(z2, how='row')\n",
    "        sigma2 = w2.T.dot(sigma3) * self._sigmoid_gradient(z2)\n",
    "        sigma2 = sigma2[1:, :]\n",
    "        grad1 = sigma2.dot(a1)\n",
    "        grad2 = sigma3.dot(a2.T)\n",
    "\n",
    "        # regularize\n",
    "        grad1[:, 1:] += self.l2 * w1[:, 1:]\n",
    "        grad1[:, 1:] += self.l1 * np.sign(w1[:, 1:])\n",
    "        grad2[:, 1:] += self.l2 * w2[:, 1:]\n",
    "        grad2[:, 1:] += self.l1 * np.sign(w2[:, 1:])\n",
    "\n",
    "        return grad1, grad2\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        y_pred : array, shape = [n_samples]\n",
    "            Predicted class labels.\n",
    "\n",
    "        \"\"\"\n",
    "        if len(X.shape) != 2:\n",
    "            raise AttributeError('X must be a [n_samples, n_features] array.\\n'\n",
    "                                 'Use X[:,None] for 1-feature classification,'\n",
    "                                 '\\nor X[[i]] for 1-sample classification')\n",
    "\n",
    "        a1, z2, a2, z3, a3 = self._feedforward(X, self.w1, self.w2)\n",
    "        y_pred = np.argmax(z3, axis=0)\n",
    "        return y_pred\n",
    "\n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        y : array, shape = [n_samples]\n",
    "            Target class labels.\n",
    "        print_progress : bool (default: False)\n",
    "            Prints progress as the number of epochs\n",
    "            to stderr.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        self.cost_ = []\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        y_enc = self._encode_labels(y, self.n_output)\n",
    "\n",
    "        delta_w1_prev = np.zeros(self.w1.shape)\n",
    "        delta_w2_prev = np.zeros(self.w2.shape)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            self.eta /= (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx = np.random.permutation(y_data.shape[0])\n",
    "                X_data, y_enc = X_data[idx], y_enc[:, idx]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                a1, z2, a2, z3, a3 = self._feedforward(X_data[idx],\n",
    "                                                       self.w1,\n",
    "                                                       self.w2)\n",
    "                cost = self._get_cost(y_enc=y_enc[:, idx],\n",
    "                                      output=a3,\n",
    "                                      w1=self.w1,\n",
    "                                      w2=self.w2)\n",
    "                self.cost_.append(cost)\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2 = self._get_gradient(a1=a1, a2=a2,\n",
    "                                                  a3=a3, z2=z2,\n",
    "                                                  y_enc=y_enc[:, idx],\n",
    "                                                  w1=self.w1,\n",
    "                                                  w2=self.w2)\n",
    "\n",
    "                delta_w1, delta_w2 = self.eta * grad1, self.eta * grad2\n",
    "                self.w1 -= (delta_w1 + (self.alpha * delta_w1_prev))\n",
    "                self.w2 -= (delta_w2 + (self.alpha * delta_w2_prev))\n",
    "                delta_w1_prev, delta_w2_prev = delta_w1, delta_w2\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.NeuralNetMLP at 0x7fc15e9c1e50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialization\n",
    "nn = NeuralNetMLP(n_output=2, \n",
    "                  n_features=X_train_count.shape[1], \n",
    "                  n_hidden=50, \n",
    "                  l2=0.1, \n",
    "                  l1=0.0, \n",
    "                  epochs=100, \n",
    "                  eta=0.001,\n",
    "                  alpha=0.001,\n",
    "                  decrease_const=0.00001,\n",
    "                  minibatches=50, \n",
    "                  shuffle=True,\n",
    "                  random_state=1)\n",
    "nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100/100"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.NeuralNetMLP at 0x7fc15e9c1e50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "nn.fit(X_train_count, y_train, print_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFupJREFUeJzt3XGwHWd93vHvg2SMYxwsg6JRJTVWJmqoTLCBW5cCkyE4\nxCakyG1TIyYkmtSt2oybQptJkMK0JTNR6yYdhtJgWhVIlUKsKsSONSRxEMI0zRQsrrENloxqge1Y\nqmRdYBwgSRUsfv3jvELHt5Z1ZWvvfeXz/cyc2XfffXfv77xj+/Hu2bMnVYUkSb15zkIXIEnSkzGg\nJEldMqAkSV0yoCRJXTKgJEldMqAkSV0aNKCS/PMke5Pcl+TmJM9LckmSXUkeaMslY+M3JzmQZH+S\nq4esTZLUtwz1PagkK4A/BtZW1V8k2QH8PrAW+FpV3ZhkE7Ckqt6RZC1wM3Al8FeATwB/raqOD1Kg\nJKlrQ1/iWwxckGQx8F3A/wHWAdva9m3Ata29DtheVceq6kHgAKOwkiRNoMVDHbiqDiX598CfAH8B\nfLyqPp5kWVUdbsOOAMtaewXwmbFDHGx9T5BkI7AR4MILL3zFi1/84qHegiRpAHfddddXqmrp6cYN\nFlDts6V1wGrgMeC3k7x1fExVVZIzusZYVVuBrQBTU1M1PT19liqWJM2HJA/PZdyQl/h+BHiwqmaq\n6lvALcCrgEeTLAdoy6Nt/CFg1dj+K1ufJGkCDRlQfwK8Msl3JQlwFXA/sBPY0MZsAG5r7Z3A+iTn\nJ1kNrAH2DFifJKljQ34GdWeSjwKfAx4H7mZ0ae75wI4k1wMPA9e18XvbnX772vgbvINPkibXYLeZ\nzwc/g5Kkc0+Su6pq6nTjfJKEJKlLBpQkqUsGlCSpSwaUJKlLBpQkqUsGlCSpSwaUJKlLBpQkqUsG\nlCSpSwaUJKlLBpQkqUsGlCSpSwaUJKlLBpQkqUsGlCSpSwaUJKlLBpQkqUsGlCSpSwaUJKlLBpQk\nqUuDBVSSH0hyz9jr60nenuSSJLuSPNCWS8b22ZzkQJL9Sa4eqjZJUv8GC6iq2l9VV1TVFcArgD8H\nbgU2Aburag2wu62TZC2wHrgMuAa4KcmioeqTJPVtvi7xXQV8qaoeBtYB21r/NuDa1l4HbK+qY1X1\nIHAAuHKe6pMkdWa+Amo9cHNrL6uqw619BFjW2iuAR8b2Odj6niDJxiTTSaZnZmaGqleStMAGD6gk\nzwXeBPz27G1VVUCdyfGqamtVTVXV1NKlS89SlZKk3szHGdQbgM9V1aNt/dEkywHa8mjrPwSsGttv\nZeuTJE2g+Qiot3Dy8h7ATmBDa28AbhvrX5/k/CSrgTXAnnmoT5LUocVDHjzJhcDrgX881n0jsCPJ\n9cDDwHUAVbU3yQ5gH/A4cENVHR+yPklSvwYNqKr6M+CFs/q+yuiuvicbvwXYMmRNkqRzg0+SkCR1\nyYCSJHXJgJIkdcmAkiR1yYCSJHXJgJIkdcmAkiR1yYCSJHXJgJIkdcmAkiR1yYCSJHXJgJIkdcmA\nkiR1yYCSJHXJgJIkdcmAkiR1yYCSJHXJgJIkdcmAkiR1yYCSJHVp0IBKcnGSjyb5YpL7k/ytJJck\n2ZXkgbZcMjZ+c5IDSfYnuXrI2iRJfRv6DOo/ALdX1YuBy4H7gU3A7qpaA+xu6yRZC6wHLgOuAW5K\nsmjg+iRJnRosoJK8APgh4IMAVfWXVfUYsA7Y1oZtA65t7XXA9qo6VlUPAgeAK4eqT5LUtyHPoFYD\nM8BvJLk7yQeSXAgsq6rDbcwRYFlrrwAeGdv/YOt7giQbk0wnmZ6ZmRmwfEnSQhoyoBYDLwfeX1Uv\nA/6MdjnvhKoqoM7koFW1taqmqmpq6dKlZ61YSVJfhgyog8DBqrqzrX+UUWA9mmQ5QFsebdsPAavG\n9l/Z+iRJE2iwgKqqI8AjSX6gdV0F7AN2Ahta3wbgttbeCaxPcn6S1cAaYM9Q9UmS+rZ44OP/HPCR\nJM8Fvgz8DKNQ3JHkeuBh4DqAqtqbZAejEHscuKGqjg9cnySpU4MGVFXdA0w9yaarTjF+C7BlyJok\nSecGnyQhSeqSASVJ6pIBJUnqkgElSeqSASVJ6pIBJUnqkgElSeqSASVJ6pIBJUnqkgElSeqSASVJ\n6pIBJUnqkgElSeqSASVJ6pIBJUnqkgElSeqSASVJ6pIBJUnqkgElSeqSASVJ6tKgAZXkoSRfSHJP\nkunWd0mSXUkeaMslY+M3JzmQZH+Sq4esTZLUt/k4g/rhqrqiqqba+iZgd1WtAXa3dZKsBdYDlwHX\nADclWTQP9UmSOrQQl/jWAdtaextw7Vj/9qo6VlUPAgeAKxegPklSB4YOqAI+keSuJBtb37KqOtza\nR4Blrb0CeGRs34Ot7wmSbEwynWR6ZmZmqLolSQts8cDHf01VHUryPcCuJF8c31hVlaTO5IBVtRXY\nCjA1NXVG+0qSzh2DnkFV1aG2PArcyuiS3aNJlgO05dE2/BCwamz3la1PkjSBBguoJBcmuehEG/hR\n4D5gJ7ChDdsA3NbaO4H1Sc5PshpYA+wZqj5JUt+GvMS3DLg1yYm/81tVdXuSzwI7klwPPAxcB1BV\ne5PsAPYBjwM3VNXxAeuTJHVssICqqi8Dlz9J/1eBq06xzxZgy1A1SZLOHT5JQpLUJQNKktQlA0qS\n1CUDSpLUJQNKktQlA0qS1CUDSpLUJQNKktQlA0qS1CUDSpLUJQNKktQlA0qS1CUDSpLUJQNKktQl\nA0qS1KU5BVSS/zaXPkmSzpa5nkFdNr6SZBHwirNfjiRJI08ZUEk2J/kG8NIkX2+vbwBHgdvmpUJJ\n0kR6yoCqqn9bVRcBv1ZV391eF1XVC6tq8zzVKEmaQHO9xPexJBcCJHlrkncn+d4B65IkTbi5BtT7\ngT9Pcjnw88CXgN+cy45JFiW5O8nH2volSXYleaAtl4yN3ZzkQJL9Sa4+w/ciSXoWmWtAPV5VBawD\nfr2q3gdcNMd93wbcP7a+CdhdVWuA3W2dJGuB9YxuyLgGuKndjCFJmkBzDahvJNkM/BTwe0meA5x3\nup2SrATeCHxgrHsdsK21twHXjvVvr6pjVfUgcAC4co71SZKeZeYaUG8GjgH/oKqOACuBX5vDfu8B\nfhH49ljfsqo63NpHgGWtvQJ4ZGzcwdb3BEk2JplOMj0zMzPH8iVJ55o5BVQLpY8AL0jy48D/raqn\n/AyqjTtaVXc9xXELqDOol6raWlVTVTW1dOnSM9lVknQOmeuTJK4D9gB/H7gOuDPJT5xmt1cDb0ry\nELAdeF2SDwOPJlnejruc0XeqAA4Bq8b2X9n6JEkTaK6X+N4J/I2q2lBVP83os6F/+VQ7VNXmqlpZ\nVZcyuvnhk1X1VmAnsKEN28DJL/zuBNYnOT/JamANo1CUJE2gxXMc95yqOjq2/lWe/oNmbwR2JLke\neJjRGRlVtTfJDmAf8DhwQ1Udf5p/Q5J0jptrQN2e5A+Bm9v6m4Hfn+sfqapPAZ9q7a8CV51i3BZg\ny1yPK0l69nrKgEry/YzuuvuFJH8XeE3b9GlGN01IkjSI051BvQfYDFBVtwC3ACT5wbbtbw9anSRp\nYp3uc6RlVfWF2Z2t79JBKpIkidMH1MVPse2Cs1mIJEnjThdQ00n+0ezOJP8QOOUXcCVJeqZO9xnU\n24Fbk/wkJwNpCngu8HeGLEySNNmeMqCq6lHgVUl+GHhJ6/69qvrk4JVJkibanL4HVVV3AHcMXIsk\nSd/xdJ8GIUnSoAwoSVKXDChJUpcMKElSlwwoSVKXDChJUpcMKElSlwwoSVKXDChJUpcMKElSlwwo\nSVKXDChJUpcGC6gkz0uyJ8m9SfYm+eXWf0mSXUkeaMslY/tsTnIgyf4kVw9VmySpf0OeQR0DXldV\nlwNXANckeSWwCdhdVWuA3W2dJGuB9cBlwDXATUkWDVifJKljgwVUjXyzrZ7XXgWsA7a1/m3Ata29\nDtheVceq6kHgAHDlUPVJkvo26GdQSRYluQc4CuyqqjuBZVV1uA05Aixr7RXAI2O7H2x9s4+5Mcl0\nkumZmZkBq5ckLaRBA6qqjlfVFcBK4MokL5m1vRidVZ3JMbdW1VRVTS1duvQsVitJ6sm83MVXVY8x\n+kXea4BHkywHaMujbdghYNXYbitbnyRpAg15F9/SJBe39gXA64EvAjuBDW3YBuC21t4JrE9yfpLV\nwBpgz1D1SZL6tnjAYy8HtrU78Z4D7KiqjyX5NLAjyfXAw8B1AFW1N8kOYB/wOHBDVR0fsD5JUscy\n+hjo3DQ1NVXT09MLXYYk6Qwkuauqpk43zidJSJK6ZEBJkrpkQEmSumRASZK6ZEBJkrpkQEmSumRA\nSZK6ZEBJkrpkQEmSumRASZK6ZEBJkrpkQEmSumRASZK6ZEBJkrpkQEmSumRASZK6ZEBJkrpkQEmS\numRASZK6ZEBJkro0WEAlWZXkjiT7kuxN8rbWf0mSXUkeaMslY/tsTnIgyf4kVw9VmySpf0OeQT0O\n/HxVrQVeCdyQZC2wCdhdVWuA3W2dtm09cBlwDXBTkkUD1idJ6thgAVVVh6vqc639DeB+YAWwDtjW\nhm0Drm3tdcD2qjpWVQ8CB4Arh6pPktS3efkMKsmlwMuAO4FlVXW4bToCLGvtFcAjY7sdbH2zj7Ux\nyXSS6ZmZmcFqliQtrMEDKsnzgd8B3l5VXx/fVlUF1Jkcr6q2VtVUVU0tXbr0LFYqSerJoAGV5DxG\n4fSRqrqldT+aZHnbvhw42voPAavGdl/Z+iRJE2jIu/gCfBC4v6rePbZpJ7ChtTcAt431r09yfpLV\nwBpgz1D1SZL6tnjAY78a+CngC0nuaX2/BNwI7EhyPfAwcB1AVe1NsgPYx+gOwBuq6viA9UmSOjZY\nQFXVHwM5xearTrHPFmDLUDVJks4dPklCktQlA0qS1CUDSpLUJQNKktQlA0qS1CUDSpLUJQNKktQl\nA0qS1CUDSpLUJQNKktQlA0qS1CUDSpLUJQNKktQlA0qS1CUDSpLUJQNKktQlA0qS1CUDSpLUJQNK\nktQlA0qS1KXBAirJh5IcTXLfWN8lSXYleaAtl4xt25zkQJL9Sa4eqi5J0rlhyDOo/wpcM6tvE7C7\nqtYAu9s6SdYC64HL2j43JVk0YG2SpM4NFlBV9UfA12Z1rwO2tfY24Nqx/u1VdayqHgQOAFcOVZsk\nqX/z/RnUsqo63NpHgGWtvQJ4ZGzcwdb3/0myMcl0kumZmZnhKpUkLagFu0miqgqop7Hf1qqaqqqp\npUuXDlCZJKkH8x1QjyZZDtCWR1v/IWDV2LiVrU+SNKHmO6B2AhtaewNw21j/+iTnJ1kNrAH2zHNt\nkqSOLB7qwEluBl4LvCjJQeBfAzcCO5JcDzwMXAdQVXuT7AD2AY8DN1TV8aFqkyT1b7CAqqq3nGLT\nVacYvwXYMlQ9kqRzi0+SkCR1yYCSJHXJgJIkdcmAkiR1yYCSJHXJgJIkdcmAkiR1yYCSJHXJgJIk\ndcmAkiR1yYCSJHXJgJIkdcmAkiR1yYCSJHXJgJIkdcmAkiR1yYCSJHXJgJIkdcmAkiR1yYCSJHWp\nu4BKck2S/UkOJNm00PVIkhZGVwGVZBHwPuANwFrgLUnWLmxVkqSF0FVAAVcCB6rqy1X1l8B2YN0C\n1yRJWgCLF7qAWVYAj4ytHwT+5viAJBuBjW31m0n2P8O/+SLgK8/wGM8WzsWI83CSc3GSczFyNubh\ne+cyqLeAOq2q2gpsPVvHSzJdVVNn63jnMudixHk4ybk4ybkYmc956O0S3yFg1dj6ytYnSZowvQXU\nZ4E1SVYneS6wHti5wDVJkhZAV5f4qurxJP8U+ENgEfChqto78J89a5cLnwWcixHn4STn4iTnYmTe\n5iFVNV9/S5KkOevtEp8kSYABJUnq1MQG1CQ8UinJh5IcTXLfWN8lSXYleaAtl4xt29zmY3+Sq8f6\nX5HkC23be5Nkvt/LM5FkVZI7kuxLsjfJ21r/JM7F85LsSXJvm4tfbv0TNxcwenpNkruTfKytT+Q8\nACR5qL2Pe5JMt76FnY+qmrgXoxswvgR8H/Bc4F5g7ULXNcD7/CHg5cB9Y32/Cmxq7U3Av2vttW0e\nzgdWt/lZ1LbtAV4JBPgD4A0L/d7OcB6WAy9v7YuA/93e7yTORYDnt/Z5wJ3t/UzcXLT38C+A3wI+\n1tYnch7a+3gIeNGsvgWdj0k9g5qIRypV1R8BX5vVvQ7Y1trbgGvH+rdX1bGqehA4AFyZZDnw3VX1\nmRr90/ebY/ucE6rqcFV9rrW/AdzP6KklkzgXVVXfbKvntVcxgXORZCXwRuADY90TNw+nsaDzMakB\n9WSPVFqxQLXMt2VVdbi1jwDLWvtUc7KitWf3n5OSXAq8jNGZw0TORbusdQ9wFNhVVZM6F+8BfhH4\n9ljfJM7DCQV8Isld7ZFysMDz0dX3oDS/qqqSTMz3DJI8H/gd4O1V9fXxS+OTNBdVdRy4IsnFwK1J\nXjJr+7N+LpL8OHC0qu5K8tonGzMJ8zDLa6rqUJLvAXYl+eL4xoWYj0k9g5rkRyo92k7Dacujrf9U\nc3KotWf3n1OSnMconD5SVbe07omcixOq6jHgDuAaJm8uXg28KclDjC7xvy7Jh5m8efiOqjrUlkeB\nWxl9FLKg8zGpATXJj1TaCWxo7Q3AbWP965Ocn2Q1sAbY007vv57kle1unJ8e2+ec0Or+IHB/Vb17\nbNMkzsXSduZEkguA1wNfZMLmoqo2V9XKqrqU0b//n6yqtzJh83BCkguTXHSiDfwocB8LPR8LfefI\nQr2AH2N0N9eXgHcudD0DvcebgcPAtxhdC74eeCGwG3gA+ARwydj4d7b52M/YnTfAVPuH9UvAr9Oe\nQHKuvIDXMLq+/nngnvb6sQmdi5cCd7e5uA/4V61/4uZi7H28lpN38U3kPDC6o/ne9tp74r+JCz0f\nPupIktSlSb3EJ0nqnAElSeqSASVJ6pIBJUnqkgElSeqSASXNQZLj7SnPJ15n7Qn4SS7N2BPnz3Df\ntOW7xtdnjRmvfedY/+okd7anTv/39p1AqRveZi7NQZJvVtXzBzr2pYy+h/OS0wx9sn3/DaPnCv4I\no+96faiq7pk15klrT7IDuKWqtif5T8C9VfX+p/EWpEF4BiU9A+03dH61/f7NniTf3/ovTfLJJJ9P\nsjvJX239y5LcmtHvMd2b5FXtUIuS/JeMfqPp4+0pDyT5Zxn9jtXnk2yf/fer6pcYParorcD7ZofT\nU9Qd4HXAR1vX+JOqpS4YUNLcXDDrEt+bx7b9aVX9IKNvzb+n9f1HYFtVvRT4CPDe1v9e4H9U1eWM\nfqtrb+tfwyhgLgMeA/5e698EvKwd55/MLirJrwC3Ax8Gbkhy+ZPU/rwkn0vymSQnQuiFwGNV9Xhb\nP5efwq1nKS/xSXPwFJfJHgJeV1Vfbg+kPVJVL0zyFWB5VX2r9R+uqhclmQFWVtWxsWNcyuhnL9a0\n9XcA51XVryS5Hfgm8LvA79bJ33I6sW+qqpK8q6redWJ91pgVNXpK9fcBnwSuAv4U+ExVnTjjWwX8\nwdO5zCgNxTMo6ZmrU7TPxLGx9nFO/hTOG4H3MTrb+mySJ/xEzokwqqp3ja/PGnPiKdVfBj7F6Pew\nvgpcPHa8c/Yp3Hr2MqCkZ+7NY8tPt/b/YvSUbICfBP5na+8Gfha+88OBLzjVQZM8B1hVVXcA7wBe\nAJzRjRpJliQ5v7VfxOhnJva1ILsD+Ik2dPxJ1VIX/MFCaW4uyOhXaE+4vapO3Gq+JMnnGZ0FvaX1\n/RzwG0l+AZgBfqb1vw3YmuR6RmdKP8voifNPZhHw4RZiAd5bo99wOhN/HfjPSb7N6H9Ib6yqfW3b\nO4Dt7XOsuxn9JInUDT+Dkp6B9hnUVFV9ZaFrkZ5tvMQnSeqSZ1CSpC55BiVJ6pIBJUnqkgElSeqS\nASVJ6pIBJUnq0v8DWW0j9GPFq4MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbd11ef8610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(range(len(nn.cost_)), nn.cost_, color='k')\n",
    "ylim([0, 800])\n",
    "ylabel('Cost')\n",
    "xlabel('Epochs * 50')\n",
    "tight_layout()\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = np.array_split(range(len(nn.cost_)), 1000)\n",
    "cost_array = np.array(nn.cost_)\n",
    "cost_averages = [np.mean(cost_array[i]) for i in batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbd115d6a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(range(len(cost_averages)), cost_averages, color='b')\n",
    "ylim([0, 300])\n",
    "ylabel('Cost')\n",
    "xlabel('Epochs')\n",
    "tight_layout()\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98580500560328721"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred = nn.predict(X_train_count)\n",
    "accuracy = \\\n",
    "  ((np.sum(y_train == y_train_pred, axis=0)).astype('float') / X_train_count.shape[0])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87407407407407411"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = nn.predict(X_test_count)\n",
    "accuracy = \\\n",
    "  ((np.sum(y_test == y_test_pred, axis=0)).astype('float') / X_test_count.shape[0])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "misclassified_img = X_test[y_test != y_test_pred][:30]\n",
    "correct_lab = y_test[y_test != y_test_pred][:30]\n",
    "misclassified_lab = y_test_pred[y_test != y_test_pred][:30]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=6, ncols=5, sharex=True, sharey=True,)\n",
    "ax = ax.flatten()\n",
    "for i in range(30):\n",
    "    img = misclassified_img[i].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "    ax[i].set_title('%d) t: %d p: %d' % (i+1, correct_lab[i], misclassified_lab[i]))\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "tight_layout()\n",
    "show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
